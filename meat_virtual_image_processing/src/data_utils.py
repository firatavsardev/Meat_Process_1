"""
Veri yÃ¼kleme, Ã¶n iÅŸleme ve augmentation fonksiyonlarÄ±.
Bu modÃ¼l et gÃ¶rÃ¼ntÃ¼lerini yÃ¼klemek ve model eÄŸitimi iÃ§in hazÄ±rlamak amacÄ±yla kullanÄ±lÄ±r.
"""

import os
import numpy as np
import pandas as pd
import cv2
from PIL import Image
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf


class MeatDataset:
    """Et veri seti yÃ¶netimi iÃ§in ana sÄ±nÄ±f."""
    
    def __init__(self, data_dir, img_size=(224, 224)):
        """
        Args:
            data_dir (str): Veri seti ana dizini
            img_size (tuple): Hedef gÃ¶rÃ¼ntÃ¼ boyutu (height, width)
        """
        self.data_dir = data_dir
        self.img_size = img_size
        self.images = []
        self.scores = []
        
    def load_from_csv(self, csv_path):
        """
        CSV dosyasÄ±ndan veri yÃ¼kler.
        CSV formatÄ±: image_path, freshness_score
        
        Args:
            csv_path (str): CSV dosya yolu
        
        Returns:
            tuple: (image_paths, scores)
        """
        df = pd.read_csv(csv_path)
        print(f"âœ“ CSV'den {len(df)} kayÄ±t yÃ¼klendi")
        
        # GÃ¶rÃ¼ntÃ¼ yollarÄ±nÄ± tam yol haline getir
        image_paths = [os.path.join(self.data_dir, path) for path in df['image_path']]
        scores = df['freshness_score'].values
        
        # SkorlarÄ±n 0-1 arasÄ±nda olduÄŸunu kontrol et
        if scores.min() < 0 or scores.max() > 1:
            print(f"âš  UyarÄ±: Skorlar 0-1 aralÄ±ÄŸÄ± dÄ±ÅŸÄ±nda! Min: {scores.min()}, Max: {scores.max()}")
            # Normalizasyon yap
            scores = (scores - scores.min()) / (scores.max() - scores.min())
            print(f"âœ“ Skorlar 0-1 aralÄ±ÄŸÄ±na normalize edildi")
        
        return image_paths, scores
    
    def create_csv_from_folders(self, folder_mapping, output_csv='data/raw/labels.csv'):
        """
        KlasÃ¶r yapÄ±sÄ±ndan CSV oluÅŸturur.
        
        Args:
            folder_mapping (dict): KlasÃ¶r adÄ± -> skor eÅŸleÅŸtirmesi
                Ã–rnek: {'fresh': 0.0, 'medium': 0.5, 'spoiled': 1.0}
            output_csv (str): OluÅŸturulacak CSV dosya yolu
        
        Returns:
            pd.DataFrame: OluÅŸturulan DataFrame
        """
        data = []
        
        for folder_name, score in folder_mapping.items():
            folder_path = os.path.join(self.data_dir, folder_name)
            
            if not os.path.exists(folder_path):
                print(f"âš  KlasÃ¶r bulunamadÄ±: {folder_path}")
                continue
            
            # KlasÃ¶rdeki tÃ¼m gÃ¶rÃ¼ntÃ¼leri bul
            image_files = [f for f in os.listdir(folder_path) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
            
            print(f"âœ“ {folder_name}: {len(image_files)} gÃ¶rÃ¼ntÃ¼ bulundu (skor: {score})")
            
            for img_file in image_files:
                rel_path = os.path.join(folder_name, img_file)
                data.append({'image_path': rel_path, 'freshness_score': score})
        
        # DataFrame oluÅŸtur ve kaydet
        df = pd.DataFrame(data)
        df.to_csv(output_csv, index=False)
        print(f"\nâœ“ CSV dosyasÄ± oluÅŸturuldu: {output_csv}")
        print(f"  Toplam {len(df)} gÃ¶rÃ¼ntÃ¼ kaydedildi")
        
        return df
    
    def preprocess_image(self, image_path):
        """
        Tek bir gÃ¶rÃ¼ntÃ¼yÃ¼ Ã¶niÅŸler.
        
        Args:
            image_path (str): GÃ¶rÃ¼ntÃ¼ dosya yolu
        
        Returns:
            np.ndarray: Ä°ÅŸlenmiÅŸ gÃ¶rÃ¼ntÃ¼ (normalized)
        """
        try:
            # GÃ¶rÃ¼ntÃ¼yÃ¼ yÃ¼kle
            img = cv2.imread(image_path)
            if img is None:
                print(f"âš  GÃ¶rÃ¼ntÃ¼ yÃ¼klenemedi: {image_path}")
                return None
            
            # RGB'ye Ã§evir (OpenCV BGR kullanÄ±r)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
            
            # Yeniden boyutlandÄ±r
            img = cv2.resize(img, self.img_size)
            
            # 0-1 aralÄ±ÄŸÄ±na normalize et
            img = img.astype(np.float32) / 255.0
            
            return img
            
        except Exception as e:
            print(f"âš  Hata ({image_path}): {e}")
            return None
    
    def load_and_preprocess(self, image_paths, scores, test_size=0.2, random_state=42):
        """
        TÃ¼m veri setini yÃ¼kler ve train/validation'a bÃ¶ler.
        
        Args:
            image_paths (list): GÃ¶rÃ¼ntÃ¼ yollarÄ± listesi
            scores (np.ndarray): Bozulma skorlarÄ±
            test_size (float): Validation oranÄ±
            random_state (int): Random seed
        
        Returns:
            tuple: (X_train, X_val, y_train, y_val)
        """
        print(f"\nğŸ“Š GÃ¶rÃ¼ntÃ¼ler yÃ¼kleniyor...")
        
        images = []
        valid_scores = []
        
        for i, (img_path, score) in enumerate(zip(image_paths, scores)):
            if (i + 1) % 100 == 0:
                print(f"  Ä°ÅŸlendi: {i+1}/{len(image_paths)}")
            
            img = self.preprocess_image(img_path)
            if img is not None:
                images.append(img)
                valid_scores.append(score)
        
        images = np.array(images)
        valid_scores = np.array(valid_scores)
        
        print(f"\nâœ“ Toplam {len(images)} gÃ¶rÃ¼ntÃ¼ baÅŸarÄ±yla yÃ¼klendi")
        print(f"  GÃ¶rÃ¼ntÃ¼ boyutu: {images.shape}")
        
        # Train/validation split
        X_train, X_val, y_train, y_val = train_test_split(
            images, valid_scores, 
            test_size=test_size, 
            random_state=random_state
        )
        
        print(f"\nğŸ“‚ Veri bÃ¶lÃ¼nmesi:")
        print(f"  Training: {len(X_train)} gÃ¶rÃ¼ntÃ¼")
        print(f"  Validation: {len(X_val)} gÃ¶rÃ¼ntÃ¼")
        
        return X_train, X_val, y_train, y_val


def get_augmentation_pipeline(rotation_range=20, 
                              width_shift_range=0.2,
                              height_shift_range=0.2,
                              horizontal_flip=True,
                              zoom_range=0.2,
                              brightness_range=(0.8, 1.2)):
    """
    Veri artÄ±rma (augmentation) pipeline'Ä± oluÅŸturur.
    
    Args:
        rotation_range (int): Rastgele dÃ¶ndÃ¼rme aÃ§Ä±sÄ±
        width_shift_range (float): Yatay kaydÄ±rma oranÄ±
        height_shift_range (float): Dikey kaydÄ±rma oranÄ±
        horizontal_flip (bool): Yatay Ã§evirme
        zoom_range (float): Zoom oranÄ±
        brightness_range (tuple): ParlaklÄ±k deÄŸiÅŸim aralÄ±ÄŸÄ±
    
    Returns:
        ImageDataGenerator: Augmentation pipeline
    """
    datagen = ImageDataGenerator(
        rotation_range=rotation_range,
        width_shift_range=width_shift_range,
        height_shift_range=height_shift_range,
        horizontal_flip=horizontal_flip,
        zoom_range=zoom_range,
        brightness_range=brightness_range,
        fill_mode='nearest'
    )
    
    return datagen


def create_tf_dataset(X, y, batch_size=16, augment=False):
    """
    TensorFlow Dataset oluÅŸturur.
    
    Args:
        X (np.ndarray): GÃ¶rÃ¼ntÃ¼ler
        y (np.ndarray): Skorlar
        batch_size (int): Batch boyutu
        augment (bool): Augmentation uygula
    
    Returns:
        tf.data.Dataset: TensorFlow dataset
    """
    dataset = tf.data.Dataset.from_tensor_slices((X, y))
    
    if augment:
        # Veri artÄ±rma fonksiyonu
        def augment_fn(image, label):
            # Random flip
            image = tf.image.random_flip_left_right(image)
            # Random brightness
            image = tf.image.random_brightness(image, max_delta=0.2)
            # Random contrast
            image = tf.image.random_contrast(image, lower=0.8, upper=1.2)
            # Clip to [0, 1]
            image = tf.clip_by_value(image, 0.0, 1.0)
            return image, label
        
        dataset = dataset.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)
    
    dataset = dataset.shuffle(buffer_size=1000)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)
    
    return dataset


if __name__ == "__main__":
    # Test kodu
    print("ğŸ§ª Data Utils Test")
    
    # Ã–rnek kullanÄ±m
    dataset = MeatDataset(data_dir='data/raw')
    
    # KlasÃ¶r bazlÄ± CSV oluÅŸturma Ã¶rneÄŸi
    # folder_mapping = {
    #     'fresh': 0.0,      # Taze
    #     'medium': 0.5,     # Orta
    #     'spoiled': 1.0     # Bozuk
    # }
    # df = dataset.create_csv_from_folders(folder_mapping)
